# Awesome-Emotion LLM and MLLM
[![Awesome](https://awesome.re/badge.svg)](https://github.com/zjunlp/ModelEditingPapers) 
![](https://img.shields.io/github/last-commit/nku-zhichengzhang/Awesome-emotion_llm_and_mllm?color=green) 
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)
![](https://img.shields.io/badge/PRs-Welcome-red) 
![](https://img.shields.io/badge/DatasetNumber-58-brightgreen)
![](https://img.shields.io/badge/PaperNumber-190-brightgreen)

<p align="center">
    <br>
    <img src="assets/logo.png"/>
    <br>
<p>



List of **Papers**, **Datasets** and **Code Repositories** for ***Affective Computing with LLM and MLLM***. This repo contains a majority of research works in the affective computing field, but it still may not encompass all the noteworthy works.


> If you find we miss some related works or have wrong clarifications, please contact us or open issues!
> 
> This repo is under W.I.P. Please feel free to open issues and make PRs!

## üìñ Table of Contents
- [Awesome-Emotion LLM and MLLM](#awesome-emotion-llm-and-mllm)
  - [üìñ Table of Contents](#-table-of-contents)
  - [üìà Survey](#-survey)
  - [üíæ Dataset](#-dataset)
  - [‚≠ê Text LLM](#-text-llm)
  - [‚≠ê Video MLLM](#-video-mllm)
  - [‚≠ê Image MLLM](#-image-mllm)
  - [‚≠ê Audio MLLM](#-audio-mllm)

## üìà Survey

- **Toward Label-Efficient Emotion and Sentiment Analysis** [[PIEEE 2023]](https://ieeexplore.ieee.org/document/10253654) ![](https://img.shields.io/badge/Survey-gray)


- **Affective Image Content Analysis: Two Decades Review and New Perspectives** [[TPAMI 2022]](https://ieeexplore.ieee.org/document/9472932) ![](https://img.shields.io/badge/Survey-gray)

- **Emotion Recognition From Multiple Modalities: Fundamentals and Methodologies** [[SPM 2021]](https://ieeexplore.ieee.org/document/9591550/) ![](https://img.shields.io/badge/Survey-gray)

- **Video Affective Content Analysis: A Survey of State-of-the-Art Methods** [[TAC 2015]](https://ieeexplore.ieee.org/document/7106468) ![](https://img.shields.io/badge/Survey-gray)

- **Aesthetics and Emotions in Images** [[SPM 2011]](https://ieeexplore.ieee.org/document/5999579/) ![](https://img.shields.io/badge/Survey-gray)


## üíæ Dataset

- **VidEmo: Affective-Tree Reasoning for Emotion-Centric Video Foundation Models** [[NeurIPS 2025]](https://arxiv.org/html/2511.02712) [[Proj]](https://zzcheng.top/VidEmo) ![](https://img.shields.io/badge/Dataset-white) ![](https://img.shields.io/badge/Text-yellow) ![](https://img.shields.io/badge/Video-red)

- **Benchmarking and bridging emotion conflicts for multimodal emotion reasoning** [[ACM MM 2025]](https://dl.acm.org/doi/pdf/10.1145/3746027.3754856?casa_token=90BrllcmXHYAAAAA:ICQhNYOEH9UQQRSrlYQVCpLqDFQTaBz74JnTs5yGPbiOLzq_uzJ2fSNYDZ3AP-hFC-K9g5VZIS6L2BA) ![](https://img.shields.io/badge/Dataset-white) ![](https://img.shields.io/badge/Text-yellow) ![](https://img.shields.io/badge/Video-red)

- **Beyond Emotion Recognition: A Multi-Turn Multimodal Emotion Understanding and Reasoning Benchmark** [[ACM MM 2025]](https://dl.acm.org/doi/pdf/10.1145/3746027.3755726?casa_token=y_OtTvc92JwAAAAA:uuDcZVKLRWDCeqC1fkCyeWg8qT8UuFjFf_UdlcnH0c9o0GF2zP85z_W-Bz5I1D4aFmFmBr8zhXuqd9c) ![](https://img.shields.io/badge/Dataset-white) ![](https://img.shields.io/badge/Text-yellow) ![](https://img.shields.io/badge/Video-red)

- **Mme-emotion: A holistic evaluation benchmark for emotional intelligence in multimodal large language models** [[Arxiv 2025]](https://arxiv.org/pdf/2508.09210?) [[Code]](https://github.com/FunAudioLLM/MME-Emotion) ![](https://img.shields.io/badge/Dataset-white) ![](https://img.shields.io/badge/Text-yellow) ![](https://img.shields.io/badge/Audio-blue) ![](https://img.shields.io/badge/Video-red)

- **CULEMO: Cultural Lenses on Emotion--Benchmarking LLMs for Cross-Cultural Emotion Understanding** [[Arxiv 2025]](https://arxiv.org/pdf/2503.10688) ![](https://img.shields.io/badge/Dataset-white) ![](https://img.shields.io/badge/Text-yellow)

- **NUS-Emo at SemEval-2024 Task 3: Instruction-Tuning LLM for Multimodal Emotion-Cause Analysis in Conversations** [[Arxiv 2024]](https://arxiv.org/pdf/2501.17261?) [[Code]](https://github.com/NUSTM/SemEval-2024_ECAC) ![](https://img.shields.io/badge/Dataset-white) ![](https://img.shields.io/badge/Image-green) ![](https://img.shields.io/badge/Text-yellow)

- **Open-vocabulary Multimodal Emotion Recognition: Dataset, Metric, and Benchmark** [[ICML 2024]](https://openreview.net/pdf?id=f1uXrAjpOH) [[Code]](https://github.com/zeroQiaoba/MERTools) ![](https://img.shields.io/badge/Dataset-white) ![](https://img.shields.io/badge/Text-yellow) ![](https://img.shields.io/badge/Audio-blue) ![](https://img.shields.io/badge/Video-red)

- **Merbench: A unified evaluation benchmark for multimodal emotion recognitio** [[Arxiv 2024]](https://arxiv.org/pdf/2401.03429) [[Code]](https://github.com/zeroQiaoba/MERTools) ![](https://img.shields.io/badge/Dataset-white) ![](https://img.shields.io/badge/Text-yellow) ![](https://img.shields.io/badge/Audio-blue) ![](https://img.shields.io/badge/Video-red)

- **Emotionqueen: A benchmark for evaluating empathy of large language models** [[ACL 2024]](https://arxiv.org/pdf/2409.13359) ![](https://img.shields.io/badge/Dataset-white) ![](https://img.shields.io/badge/Text-yellow)

- **Emobench: Evaluating the emotional intelligence of large language models** [[ACL 2024]](https://aclanthology.org/2024.acl-long.326.pdf) [[Code]](https://github.com/Sahandfer/EmoBench) ![](https://img.shields.io/badge/Dataset-white) ![](https://img.shields.io/badge/Text-yellow)

- **Mer 2024: Semi-supervised learning, noise robustness, and open-vocabulary multimodal emotion recognition** [[ACM MRAC 2024]](https://dl.acm.org/doi/pdf/10.1145/3689092.3689959) [[Code]](https://github.com/zeroQiaoba/MERTools) ![](https://img.shields.io/badge/Dataset-white) ![](https://img.shields.io/badge/Text-yellow) ![](https://img.shields.io/badge/Audio-blue) ![](https://img.shields.io/badge/Video-red)

- **Mer 2023: Multi-label learning, modality robustness, and semi-supervised learning** [[ACM MM 2023]](https://dl.acm.org/doi/pdf/10.1145/3581783.3612836) [[Code]](https://github.com/zeroQiaoba/MERTools) ![](https://img.shields.io/badge/Dataset-white) ![](https://img.shields.io/badge/Text-yellow) ![](https://img.shields.io/badge/Audio-blue) ![](https://img.shields.io/badge/Video-red)

- **Explainable Multimodal Emotion Recognition** [[Arxiv 2023]](https://arxiv.org/pdf/2306.15401) [[Code]](https://github.com/zeroQiaoba/MERTools) ![](https://img.shields.io/badge/Dataset-white) ![](https://img.shields.io/badge/Text-yellow) ![](https://img.shields.io/badge/Audio-blue) ![](https://img.shields.io/badge/Video-red)

- **EQ-Bench: An Emotional Intelligence Benchmark for Large Language Models** [[Arxiv 2023]](https://arxiv.org/pdf/2312.06281) [[Code]](https://github.com/EQ-bench/EQ-Bench) ![](https://img.shields.io/badge/Dataset-white) ![](https://img.shields.io/badge/Text-yellow)

- **Celebv-text: A large-scale facial text-video dataset** [[CVPR 2023]](https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_CelebV-Text_A_Large-Scale_Facial_Text-Video_Dataset_CVPR_2023_paper.pdf) [[Code]](https://github.com/celebv-text/CelebV-Text) ![](https://img.shields.io/badge/Dataset-white) ![](https://img.shields.io/badge/Text-yellow) ![](https://img.shields.io/badge/Audio-blue) ![](https://img.shields.io/badge/Video-red)

- **Mafw: A large-scale, multi-modal, compound affective database for dynamic facial expression recognition in the wild** [[ACM MM 2022]](https://dl.acm.org/doi/pdf/10.1145/3503161.3548190) [[Code]](https://github.com/MAFW-database/MAFW) ![](https://img.shields.io/badge/Dataset-white) ![](https://img.shields.io/badge/Text-yellow) ![](https://img.shields.io/badge/Audio-blue) ![](https://img.shields.io/badge/Video-red)

- **Ferv39k: A large-scale multi-scene dataset for facial expression recognition in videos** [[CVPR 2022]](https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_FERV39k_A_Large-Scale_Multi-Scene_Dataset_for_Facial_Expression_Recognition_in_CVPR_2022_paper.pdf) [[Code]](https://github.com/wangyanckxx/FERV39k) ![](https://img.shields.io/badge/Dataset-white) ![](https://img.shields.io/badge/Text-yellow) ![](https://img.shields.io/badge/Audio-blue) ![](https://img.shields.io/badge/Video-red)

- **CelebV-HQ: A large-scale video facial attributes dataset** [[ECCV 2022]](https://arxiv.org/pdf/2207.12393) [[Code]](https://github.com/CelebV-HQ/CelebV-HQ) ![](https://img.shields.io/badge/Dataset-white) ![](https://img.shields.io/badge/Text-yellow) ![](https://img.shields.io/badge/Audio-blue) ![](https://img.shields.io/badge/Video-red)

- **Multimodal Event-Aware Network for Sentiment Analysis in Tourism** [[IEEE MultiMedia 2021]](https://ieeexplore.ieee.org/abstract/document/9428534/) [[Code]](https://github.com/wlj961012/Multi-Modal-Event-awareNetwork-for-SentimentAnalysis-in-Tourism) ![](https://img.shields.io/badge/Dataset-white) ![](https://img.shields.io/badge/Image-green) ![](https://img.shields.io/badge/Text-yellow)

- **Dfew: A large-scale database for recognizing dynamic facial expressions in the wild** [[ACM MM 2020]](https://dl.acm.org/doi/pdf/10.1145/3394171.3413620?casa_token=uCe4iKvfFcAAAAAA:Z4-1RFbS_YHJTFGWut6FwizU3ZqVae5osZR60h1faTOFyIeNkHr84hFfeYjI7eYn6jIF-NykDUIlSd4) [[Code]](https://github.com/jiangxingxun/DFEW) ![](https://img.shields.io/badge/Dataset-white) ![](https://img.shields.io/badge/Text-yellow) ![](https://img.shields.io/badge/Audio-blue) ![](https://img.shields.io/badge/Video-red)

- **H-SIMS: A Chinese Multimodal Sentiment Analysis Dataset with Fine-grained Annotation of Modality** [[ACL 2020]](https://aclanthology.org/2020.acl-main.343.pdf) [[Code]](https://github.com/thuiar/MMSA) ![](https://img.shields.io/badge/Dataset-white) ![](https://img.shields.io/badge/Text-yellow) ![](https://img.shields.io/badge/Audio-blue) ![](https://img.shields.io/badge/Video-red)

- **Mead: A large-scale audio-visual dataset for emotional talking-face generation** [[ECCV 2020]](https://link.springer.com/chapter/10.1007/978-3-030-58589-1_42) [[Code]](https://github.com/uniBruce/Mead) ![](https://img.shields.io/badge/Dataset-white) ![](https://img.shields.io/badge/Text-yellow) ![](https://img.shields.io/badge/Audio-blue) ![](https://img.shields.io/badge/Video-red)

- **Lucfer: A large-scale context-sensitive image dataset for deep learning of visual emotions** [[WACV 2019]](https://ieeexplore.ieee.org/abstract/document/8659233/) ![](https://img.shields.io/badge/Dataset-white) ![](https://img.shields.io/badge/Image-green)

- **Learning discriminative sentiment representation from strongly-and weakly supervised cnns** [[ACM TOMM 2019]](https://dl.acm.org/doi/pdf/10.1145/3326335?casa_token=l6w0tyPKzhIAAAAA:T0w_5gXpKNJZEoOMr8A7gtKtPAGfFo-5NyNk9EkWcJHp3DL-Yta789_KNWNtOxczUxX6bqt4l0qMg_g) ![](https://img.shields.io/badge/Dataset-white) ![](https://img.shields.io/badge/Image-green)

- **Multi-Interactive Memory Network for Aspect Based Multimodal Sentiment Analysis** [[AAAI 2019]](https://www.aaai.org/ojs/index.php/AAAI/article/view/3807/3685) [[Code]](https://github.com/xunan0812/MIMN) ![](https://img.shields.io/badge/Dataset-white) ![](https://img.shields.io/badge/Image-green) ![](https://img.shields.io/badge/Text-yellow)

- **Meld: A multimodal multi-party dataset for emotion recognition in conversations** [[ACL 2019]](https://aclanthology.org/P19-1050.pdf) [[Code]](https://affective-meld.github.io/) ![](https://img.shields.io/badge/Dataset-white) ![](https://img.shields.io/badge/Text-yellow) ![](https://img.shields.io/badge/Audio-blue) ![](https://img.shields.io/badge/Video-red)

- **Vistanet: Visual aspect attention network for multimodal sentiment analysis** [[AAAI 2019]](https://drive.google.com/file/d/12d8SZiNeKFgIGmO5VHSrZV2jkgwYZpNp) [[Code]](https://github.com/PreferredAI/vista-net) ![](https://img.shields.io/badge/Dataset-white) ![](https://img.shields.io/badge/Image-green) ![](https://img.shields.io/badge/Text-yellow)

- **ulti-attention recurrent network for human communication comprehension** [[AAAI 2018]](https://ojs.aaai.org/index.php/AAAI/article/view/12024/11883) [[Code]](https://github.com/matsuolab/CMU-MultimodalSDK) ![](https://img.shields.io/badge/Dataset-white) ![](https://img.shields.io/badge/Text-yellow) ![](https://img.shields.io/badge/Audio-blue) ![](https://img.shields.io/badge/Video-red)

- **Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph** [[ACL 2018]](https://aclanthology.org/P18-1208/) [[Code]](https://github.com/matsuolab/CMU-MultimodalSDK) ![](https://img.shields.io/badge/Dataset-white) ![](https://img.shields.io/badge/Text-yellow) ![](https://img.shields.io/badge/Audio-blue) ![](https://img.shields.io/badge/Video-red)

- **Contemplating visual emotions: Understanding and overcoming dataset bias** [[ECCV 2018]](https://openaccess.thecvf.com/content_ECCV_2018/papers/Rameswar_Panda_Contemplating_Visual_Emotions_ECCV_2018_paper.pdf) ![](https://img.shields.io/badge/Dataset-white) ![](https://img.shields.io/badge/Image-green)

- **Heterogeneous knowledge transfer in video emotion recognition** [[TAFFC 2018]](https://arxiv.org/pdf/1511.04798) [[Code]](http://bigvid.fudan.edu.cn/data/Ekman.zip) ![](https://img.shields.io/badge/Dataset-white) ![](https://img.shields.io/badge/Audio-blue) ![](https://img.shields.io/badge/Video-red)

- **Emotional attention: A study of image sentiment and visual attention** [[CVPR 2018]](http://openaccess.thecvf.com/content_cvpr_2018/papers/Fan_Emotional_Attention_A_CVPR_2018_paper.pdf) ![](https://img.shields.io/badge/Dataset-white) ![](https://img.shields.io/badge/Image-green)

- **Learning visual sentiment distributions via augmented conditional probability neural network** [[AAAI 2017]](https://ojs.aaai.org/index.php/AAAI/article/view/10485/10344) ![](https://img.shields.io/badge/Dataset-white) ![](https://img.shields.io/badge/Image-green)

- **Emotion recognition in context** [[CVPR 2017]](https://openaccess.thecvf.com/content_cvpr_2017/papers/Kosti_Emotion_Recognition_in_CVPR_2017_paper.pdf) [[Code]](https://github.com/rkosti/emotic) ![](https://img.shields.io/badge/Dataset-white) ![](https://img.shields.io/badge/Image-green)

- **Cross-media learning for image sentiment analysis in the wild** [[ICCVW 2017]](http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w5/Vadicamo_Cross-Media_Learning_for_ICCV_2017_paper.pdf) ![](https://img.shields.io/badge/Dataset-white) ![](https://img.shields.io/badge/Image-green)

- **Towards using visual attributes to infer image sentiment of social events** [[IJCNN 2017]](https://faculty.cc.gatech.edu/~irfan/p/2017-Ahsan-TUVAIISSE.pdf) ![](https://img.shields.io/badge/Dataset-white) ![](https://img.shields.io/badge/Image-green)

- **Predicting personalized emotion perceptions of social images** [[ACM MM 2016]](https://dl.acm.org/doi/pdf/10.1145/2964284.2964289?casa_token=AzoTZj2pFpAAAAAA:3tpXI9SGbsHrxJCQskNrqMgAAN3ntD9lXY-CWlzEKzPrDC22jc7Nam75sbcVfU21SIfAl9rXqgeeNSE) ![](https://img.shields.io/badge/Dataset-white) ![](https://img.shields.io/badge/Image-green)

- **Image sentiment analysis using latent correlations among visual, textual, and sentiment views** [[ICASSP 2016]](https://mm.doshisha.ac.jp/senti/icassp2016katsurai.pdf) ![](https://img.shields.io/badge/Dataset-white) ![](https://img.shields.io/badge/Image-green)

- **Building a large scale dataset for image emotion recognition: The fine print and the benchmark** [[AAAI 2016]](https://ojs.aaai.org/index.php/AAAI/article/view/9987/9846) ![](https://img.shields.io/badge/Dataset-white) ![](https://img.shields.io/badge/Image-green)

- **Recognizing emotions from abstract paintings using non-linear matrix completion** [[CVPR 2016]](http://openaccess.thecvf.com/content_cvpr_2016/papers/Alameda-Pineda_Recognizing_Emotions_From_CVPR_2016_paper.pdf) [[Code]](https://github.com/xavirema/nlmc) ![](https://img.shields.io/badge/Dataset-white) ![](https://img.shields.io/badge/Image-green)

- **Robust image sentiment analysis using progressively trained and domain transferred deep networks** [[AAAI 2015]](https://ojs.aaai.org/index.php/AAAI/article/view/9179/9038) ![](https://img.shields.io/badge/Dataset-white) ![](https://img.shields.io/badge/Image-green)

- **LIRIS-ACCEDE: A video database for affective content analysis** [[TAFFC 2015]](https://hal.science/hal-01375518/document) [[Code]](https://liris-accede.ec-lyon.fr/) ![](https://img.shields.io/badge/Dataset-white) ![](https://img.shields.io/badge/Audio-blue) ![](https://img.shields.io/badge/Video-red)

- **Visual affect around the world: A large-scale multilingual visual sentiment ontology** [[ACM MM 2015]](https://dl.acm.org/doi/pdf/10.1145/2733373.2806246) ![](https://img.shields.io/badge/Dataset-white) ![](https://img.shields.io/badge/Image-green)

- **A mixed bag of emotions: Model, predict, and transfer emotion distributions** [[CVPR 2015]](https://openaccess.thecvf.com/content_cvpr_2015/papers/Peng_A_Mixed_Bag_2015_CVPR_paper.pdf) ![](https://img.shields.io/badge/Dataset-white) ![](https://img.shields.io/badge/Image-green)

- **How do your friends on social media disclose your emotions** [[AAAI 2014]](https://ojs.aaai.org/index.php/AAAI/article/download/8740/8599) ![](https://img.shields.io/badge/Dataset-white) ![](https://img.shields.io/badge/Image-green)

- **Predicting emotions in user-generated videos** [[AAAI 2014]](https://ojs.aaai.org/index.php/AAAI/article/download/8724/8583) [[Code]](http://www.yugangjiang.info/research/VideoEmotions/index.html) ![](https://img.shields.io/badge/Dataset-white) ![](https://img.shields.io/badge/Audio-blue) ![](https://img.shields.io/badge/Video-red)

- **Why we watch the news: a dataset for exploring sentiment in broadcast video news** [[ACM ICMI 2014]](https://dl.acm.org/doi/pdf/10.1145/2663204.2663237?casa_token=OYZp6QYAJLYAAAAA:eePdhtSXxWzsFlElPKtmmotM5StLLzQg7ONbcLwQacIeq14aGaxs33WRBDGO9TB-ygI3J97uwi0-ks0) ![](https://img.shields.io/badge/Dataset-white) ![](https://img.shields.io/badge/Text-yellow) ![](https://img.shields.io/badge/Audio-blue) ![](https://img.shields.io/badge/Video-red)

- **Large-scale visual sentiment ontology and detectors using adjective noun pairs** [[ACM MM 2013]](https://dl.acm.org/doi/pdf/10.1145/2502081.2502282?casa_token=hSFh3fmOu18AAAAA:KxO4GwGQEq9eFw_D_ZwS7YrgFGF5VIIB2zoWTU6dJuGtb32QAYm3zcYzerWR6Wvt5-DtJa_x0r4eMvw) ![](https://img.shields.io/badge/Dataset-white) ![](https://img.shields.io/badge/Image-green)

- **Youtube movie reviews: Sentiment analysis in an audio-visual context** [[IEEE Intelligent Systems 2013]](https://opus.bibliothek.uni-augsburg.de/opus4/files/72633/72633.pdf) [[Code]](http://multicomp.cs.cmu.edu/resources/ict-mmmo-dataset/) ![](https://img.shields.io/badge/Dataset-white) ![](https://img.shields.io/badge/Text-yellow) ![](https://img.shields.io/badge/Audio-blue) ![](https://img.shields.io/badge/Video-red)

- **Utterance-level multimodal sentiment analysis.** [[ACL 2013]](https://aclanthology.org/P13-1096.pdf) ![](https://img.shields.io/badge/Dataset-white) ![](https://img.shields.io/badge/Text-yellow) ![](https://img.shields.io/badge/Audio-blue) ![](https://img.shields.io/badge/Video-red)

- **Deap: A database for emotion analysis; using physiological signals** [[TAFFC 2011]](https://access.archive-ouverte.unige.ch/access/metadata/1d5ce2a8-00d8-4793-8943-cf9e8aa56693/download) [[Code]](https://www.eecs.qmul.ac.uk/mmv/datasets/deap/) ![](https://img.shields.io/badge/Dataset-white) ![](https://img.shields.io/badge/Image-green) ![](https://img.shields.io/badge/Audio-blue)

- **The semaine database: Annotated multimodal records of emotionally colored conversations between a person and a limited agent** [[TAFFC 2011]](https://pure.qub.ac.uk/files/9746839/IEEE_Transactions_on_Affective_Computing_2012_McKeown.pdf) ![](https://img.shields.io/badge/Dataset-white) ![](https://img.shields.io/badge/Text-yellow) ![](https://img.shields.io/badge/Audio-blue) ![](https://img.shields.io/badge/Video-red)

- **The geneva affective picture database (gaped): a new 730-picture database focusing on valence and normative significance** [[BRM 2011]](https://link.springer.com/content/pdf/10.3758/s13428-011-0064-1.pdf) ![](https://img.shields.io/badge/Dataset-white) ![](https://img.shields.io/badge/Image-green)

- **Affective image classification using features inspired by psychology and art theory** [[ACM MM 2010]](https://dl.acm.org/doi/pdf/10.1145/1873951.1873965?casa_token=7rXzBGmIm5MAAAAA:Iod-rmF_K8FhZt77VMx69LLfK5Bs-goHjhmp7WltMNduhiS24dDlhNIDIsa3bZV8W4ImdZSvjJ4qPCY) ![](https://img.shields.io/badge/Dataset-white) ![](https://img.shields.io/badge/Image-green)

- **IEMOCAP: Interactive emotional dyadic motion capture database** [[Language resources and evaluation 2008]](https://sail.usc.edu/publications/files/bussolre2008.pdf) [[Code]](https://sail.usc.edu/iemocap/) ![](https://img.shields.io/badge/Dataset-white) ![](https://img.shields.io/badge/Text-yellow) ![](https://img.shields.io/badge/Audio-blue) ![](https://img.shields.io/badge/Video-red)

- **The eNTERFACE'05 audio-visual emotion database** [[ICDEW 2006]](https://scholar.archive.org/work/z4vycestq5cetadizxtwpzpfqu/access/wayback/http://poseidon.csd.auth.gr:80/papers/PUBLISHED/CONFERENCE/pdf/Martin06a.pdf) ![](https://img.shields.io/badge/Dataset-white) ![](https://img.shields.io/badge/Text-yellow) ![](https://img.shields.io/badge/Audio-blue) ![](https://img.shields.io/badge/Video-red)

- **Emotional category data on images from the international affective picture system** [[BRM 2005]](https://link.springer.com/content/pdf/10.3758/BF03192732.pdf) [[Code]](https://github.com/fnielsen/iaps) ![](https://img.shields.io/badge/Dataset-white) ![](https://img.shields.io/badge/Image-green)

- **International affective picture system (iaps): Technical manual and affective ratings** [[NIMH Center for the Study of Emotion and Attention 1993]](https://acordo.net/acordo/wp-content/uploads/2020/08/instructions.pdf) [[Code]](https://github.com/fnielsen/iaps) ![](https://img.shields.io/badge/Dataset-white) ![](https://img.shields.io/badge/Image-green)

## ‚≠ê Text LLM

- **LaERC-S: Improving LLM-based Emotion Recognition in Conversation with Speaker Characteristics** [[ACL 2025]](https://aclanthology.org/2025.coling-main.451.pdf) ![](https://img.shields.io/badge/Text-yellow)

- **Emotional Support with LLM-based Empathetic Dialogue Generation** [[Arxiv 2025]](https://arxiv.org/pdf/2507.12820?) ![](https://img.shields.io/badge/Text-yellow)

- **Exploring Emotion-Sensitive LLM-Based Conversational AI** [[Arxiv 2025]](https://arxiv.org/pdf/2502.08920) ![](https://img.shields.io/badge/Text-yellow)

- **LLM-Enhanced Multi-Teacher Knowledge Distillation for Modality-Incomplete Emotion Recognition in Daily Healthcare** [[JBHI 2025]](https://ieeexplore.ieee.org/abstract/document/10697478) ![](https://img.shields.io/badge/Text-yellow)

- **Emotional Cognitive Modeling Framework with Desire-Driven Objective Optimization for LLM-empowered Agent in Social Simulation** [[Arxiv 2025]](https://arxiv.org/pdf/2510.13195?) ![](https://img.shields.io/badge/Text-yellow)

- **MultiAgentESC: A LLM-based Multi-Agent Collaboration Framework for Emotional Support Conversation** [[ACL 2025]](https://aclanthology.org/2025.emnlp-main.232.pdf) [[Code]](https://github.com/MindIntLab-HFUT/MultiAgentESC) ![](https://img.shields.io/badge/Text-yellow)

- **Predicting Stock Price Movement with LLM-Enhanced Tweet Emotion Analysis** [[Arxiv 2025]](https://arxiv.org/pdf/2510.03633) ![](https://img.shields.io/badge/Text-yellow)

- **Revise, reason, and recognize: Llm-based emotion recognition via emotion-specific prompts and asr error correction** [[ICASSP 2025]](https://arxiv.org/pdf/2409.15551?) ![](https://img.shields.io/badge/Text-yellow)

- **Beyond silent letters: Amplifying llms in emotion recognition with vocal nuances** [[ACL 2025]](https://aclanthology.org/2025.findings-naacl.117.pdf) [[Code]](https://github.com/zehuiwu/SpeechCueLLM) ![](https://img.shields.io/badge/Text-yellow)

- **Interview-based Depression Detection Using LLM-based Text Restatement and Emotion Lexicon** [[TAFFC 2025]](https://ieeexplore.ieee.org/abstract/document/11214261) ![](https://img.shields.io/badge/Text-yellow)

- **EQ-Negotiator: An Emotion-Reasoning LLM Agent in Credit Dialogues** [[Arxiv 2025]](https://arxiv.org/pdf/2503.21080?) [[Code]](https://github.com/Yunbo-max/EmoDebt) ![](https://img.shields.io/badge/Text-yellow)

- **SEALR: Sequential Emotion-Aware LLM-Based Personalized Recommendation System** [[ACM SIGIR 2025]](https://dl.acm.org/doi/pdf/10.1145/3726302.3730249) ![](https://img.shields.io/badge/Text-yellow)

- **From personas to talks: Revisiting the impact of personas on llm-synthesized emotional support conversations** [[ACL 2025]](https://aclanthology.org/2025.emnlp-main.277.pdf) ![](https://img.shields.io/badge/Text-yellow)

- **Eeg emotion copilot: Optimizing lightweight llms for emotional eeg interpretation with assisted medical record generation** [[Neural Networks 2025]](https://arxiv.org/pdf/2410.00166) [[Code]](https://github.com/NZWANG/EEG_Emotion_Copilot) ![](https://img.shields.io/badge/Text-yellow)

- **Emotionally Aware or Tone-Deaf? Evaluating Emotional Alignment in LLM-Based Conversational Recommendation Systems** [[ACL 2025]](https://aclanthology.org/2025.winlp-main.26.pdf) ![](https://img.shields.io/badge/Text-yellow)

- **Do large language models have ‚Äúemotion neurons‚Äù? investigating the existence and role** [[ACL 2025]](https://aclanthology.org/2025.findings-acl.806.pdf) ![](https://img.shields.io/badge/Text-yellow)

- **Designing Heterogeneous LLM Agents for Financial Sentiment Analysis** [[ACM TMIS 2025]](https://dl.acm.org/doi/pdf/10.1145/3688399) ![](https://img.shields.io/badge/Text-yellow)

- **DialogXpert: Driving Intelligent and Emotion-Aware Conversations through Online Value-Based Reinforcement Learning with LLM Priors** [[Arxiv 2025]](https://arxiv.org/pdf/2505.17795) [[Code]](https://github.com/declare-lab/dialogxpert/) ![](https://img.shields.io/badge/Text-yellow)

- **TOOL-ED: Enhancing empathetic response generation with the tool calling capability of LLM** [[ACL 2025]](https://aclanthology.org/2025.coling-main.355.pdf) [[Code]](https://github.com/caohy123/EKTC) ![](https://img.shields.io/badge/Text-yellow)

- **Employing large language models for emotion detection in psychotherapy transcripts** [[frontiersin 2025]](https://www.frontiersin.org/journals/psychiatry/articles/10.3389/fpsyt.2025.1504306/pdf) ![](https://img.shields.io/badge/Text-yellow)

- **Fearful falcons and angry llamas: Emotion category annotations of arguments by humans and llms** [[ACL 2025]](https://aclanthology.org/2025.nlp4dh-1.52.pdf) ![](https://img.shields.io/badge/Text-yellow)

- **Mechanistic Interpretability of Emotion Inference in Large Language Models** [[Arxiv 2025]](https://arxiv.org/pdf/2502.05489?) [[Code]](https://github.com/aminbana/emo-llm) ![](https://img.shields.io/badge/Text-yellow)

- **Structured Prompting and LLM Ensembling for Multimodal Conversational Aspect-based Sentiment Analysis** [[ACM MM 2025]](https://dl.acm.org/doi/pdf/10.1145/3746027.3762070) ![](https://img.shields.io/badge/Text-yellow)

- **iPET: An Interactive Emotional Companion Dialogue System with LLM-Powered Virtual Pet World Simulation** [[ACL 2025]](https://aclanthology.org/2025.acl-demo.40.pdf) [[Code]](https://aclanthology.org/2025.acl-demo.40.pdf) ![](https://img.shields.io/badge/Text-yellow)

- **Fine-Grained Emotion Recognition with In-Context Learning: A Prototype Theory Approach** [[Arxiv 2025]](https://openreview.net/pdf?id=EVg9lwHFJs) ![](https://img.shields.io/badge/Text-yellow)

- **Combining Graph NN and LLM for Improved Text-Based Emotion Recognition** [[AIMSA 2024]](https://link.springer.com/chapter/10.1007/978-3-031-81542-3_12) ![](https://img.shields.io/badge/Text-yellow)

- **Emotion-Aware Systems for Hazardous Operations: Integrating Mobile Sensors with LLM** [[RICAI 2024]](https://ieeexplore.ieee.org/abstract/document/10911410) ![](https://img.shields.io/badge/Text-yellow)

- **The Effect of LLM-Based NPC Emotional States on Player Emotions: An Analysis of Interactive Game Play** [[CoG 2024]](https://ieeexplore.ieee.org/abstract/document/10645631/) ![](https://img.shields.io/badge/Text-yellow)

- **Both matter: Enhancing the emotional intelligence of large language models without compromising the general intelligence** [[ACL 2024]](https://arxiv.org/pdf/2402.10073) [[Code]](https://github.com/circle-hit/MoEI) ![](https://img.shields.io/badge/Text-yellow)

- **Empathizing Before Generation: A Double-Layered Framework for Emotional Support LLM** [[PRCV 2024]](https://www.researchgate.net/profile/Jiahao-Zhu-23/publication/385623961_Empathizing_Before_Generation_A_Double-Layered_Framework_for_Emotional_Support_LLM/links/67a95cbb461fb56424d3261f/Empathizing-Before-Generation-A-Double-Layered-Framework-for-Emotional-Support-LLM.pdf) [[Code]](https://github.com/TheodorAI/EBG) ![](https://img.shields.io/badge/Text-yellow)

- **Fairmindsim: Alignment of behavior, emotion, and belief in humans and llm agents amid ethical dilemmas** [[Arxiv 2024]](https://arxiv.org/pdf/2410.10398?) ![](https://img.shields.io/badge/Text-yellow)

- **Towards a generative approach for emotion detection and reasoning** [[Arxiv 2024]](https://arxiv.org/pdf/2408.04906) ![](https://img.shields.io/badge/Text-yellow)

- **A pilot study of measuring emotional response and perception of LLM-generated questionnaire and human-generated questionnaires** [[Scientific Reports 2024]](https://www.nature.com/articles/s41598-024-53255-1.pdf) ![](https://img.shields.io/badge/Text-yellow)

- **Apathetic or Empathetic? Evaluating {LLM}s' Emotional Alignments with Humans** [[NeurIPS 2024]](https://proceedings.neurips.cc/paper_files/paper/2024/file/b0049c3f9c53fb06f674ae66c2cf2376-Paper-Conference.pdf) [[Code]](https://github.com/CUHK-ARISE/EmotionBench) ![](https://img.shields.io/badge/Text-yellow)

- **Emollms: A series of emotional large language models and annotation tools for comprehensive affective analysis** [[ACM SIGKDD 2024]](https://dl.acm.org/doi/pdf/10.1145/3637528.3671552) [[Code]](https://github.com/lzw108/EmoLLMs/) ![](https://img.shields.io/badge/Text-yellow)

- **Emotion-Aware Embedding Fusion in LLMs (Flan-T5, LLAMA 2, DeepSeek-R1, and ChatGPT 4) for Intelligent Response Generation** [[Arxiv 2024]](https://arxiv.org/pdf/2410.01306?) ![](https://img.shields.io/badge/Text-yellow)

- **Emergence of Hierarchical Emotion Representations in Large Language Models** [[NeurIPSW 2024]](https://arxiv.org/pdf/2507.10599) ![](https://img.shields.io/badge/Text-yellow)

- **EAI: Emotional decision-making of LLMs in strategic games and ethical dilemmas** [[NeurIPS 2024]](https://proceedings.neurips.cc/paper_files/paper/2024/file/611e84703eac7cc03f78339df8aae2ed-Paper-Conference.pdf) [[Code]](https://github.com/AIRI-Institute/EAI-Framework) ![](https://img.shields.io/badge/Text-yellow)

- **Customising general large language models for specialised emotion recognition tasks** [[ICASSP 2024]](https://arxiv.org/pdf/2310.14225) ![](https://img.shields.io/badge/Text-yellow)

- **Large Language Models Performance Comparison of Emotion and Sentiment Classification** [[ACM MS 2024]](https://dl.acm.org/doi/pdf/10.1145/3603287.3651183) ![](https://img.shields.io/badge/Text-yellow)

- **Enhancing large language model with decomposed reasoning for emotion cause pair extraction** [[Arxiv 2024]](https://arxiv.org/pdf/2401.17716) ![](https://img.shields.io/badge/Text-yellow)

- **PetKaz at SemEval-2024 task 3: Advancing emotion classification with an LLM for emotion-cause pair extraction in conversations** [[ACL 2024]](https://arxiv.org/pdf/2404.05502) [[Code]](https://github.com/sachertort/petkaz-semeval-ecac) ![](https://img.shields.io/badge/Text-yellow)

- **TEII: Think, Explain, Interact and Iterate with Large Language Models to Solve Cross-lingual Emotion Detection** [[ACL 2024]](https://arxiv.org/pdf/2405.17129) ![](https://img.shields.io/badge/Text-yellow)

- **Esc-eval: Evaluating emotion support conversations in large language models** [[ACL 2024]](https://aclanthology.org/2024.emnlp-main.883.pdf) [[Code]](https://github.com/haidequanbu/ESC-Eval) ![](https://img.shields.io/badge/Text-yellow)

- **Empo: Emotion grounding for empathetic response generation through preference optimization** [[Arxiv 2024]](https://arxiv.org/pdf/2406.19071) ![](https://img.shields.io/badge/Text-yellow)

- **EEG emotion copilot: pruning LLMs for emotional EEG interpretation with assisted medical record generation** [[Arxiv 2024]](https://www.researchgate.net/profile/Nizhuan-Wang/publication/384563250_EEG_Emotion_Copilot_Pruning_LLMs_for_Emotional_EEG_Interpretation_with_Assisted_Medical_Record_Generation/links/66fe152fb753fa724d56fc14/EEG-Emotion-Copilot-Pruning-LLMs-for-Emotional-EEG-Interpretation-with-Assisted-Medical-Record-Generation.pdf) ![](https://img.shields.io/badge/Text-yellow)

- **Language models (mostly) do not consider emotion triggers when predicting emotion** [[ACL 2024]](https://aclanthology.org/2024.naacl-short.51.pdf) ![](https://img.shields.io/badge/Text-yellow)

- **Enhancing emotional generation capability of large language models via emotional chain-of-thought** [[Arxiv 2024]](https://arxiv.org/pdf/2401.06836) ![](https://img.shields.io/badge/Text-yellow)

- **Emotionally numb or empathetic? evaluating how llms feel using emotionbench** [[Arxiv 2023]](https://arxiv.org/pdf/2308.03656) ![](https://img.shields.io/badge/Text-yellow)

- **Building emotional support chatbots in the era of llms** [[Arxiv 2023]](https://arxiv.org/pdf/2308.11584) ![](https://img.shields.io/badge/Text-yellow)

- **Bias in emotion recognition with ChatGPT** [[Arxiv 2023]](https://arxiv.org/pdf/2310.11753) ![](https://img.shields.io/badge/Text-yellow)


## ‚≠ê Video MLLM

- **VidEmo: Affective-Tree Reasoning for Emotion-Centric Video Foundation Models** [[NeurIPS 2025]](https://zzcheng.top/assets/pdf/2025_NeurIPS_VidEmo.pdf) [[Code]](https://github.com/KwaiVGI/VidEmo) ![](https://img.shields.io/badge/Text-yellow) ![](https://img.shields.io/badge/Video-red)

- **Embedding Analogies for Evaluating Emotion in LLM-Generated Utterances** [[CEUR-WS 2025]](https://ceur-ws.org/Vol-4073/BEHAIV2025_CRV_3.pdf) ![](https://img.shields.io/badge/Text-yellow) ![](https://img.shields.io/badge/Video-red)

- **Reliable Learning From LLM Features for Multimodal Emotion and Intent Joint Understanding** [[ICASSP 2025]](https://ieeexplore.ieee.org/abstract/document/10888958) ![](https://img.shields.io/badge/Text-yellow) ![](https://img.shields.io/badge/Audio-blue) ![](https://img.shields.io/badge/Video-red)

- **Affectgpt: A new dataset, model, and benchmark for emotion understanding with multimodal large language models** [[ICML 2025]](https://arxiv.org/pdf/2501.16566) [[Code]](https://github.com/zeroQiaoba/AffectGPT) ![](https://img.shields.io/badge/Text-yellow) ![](https://img.shields.io/badge/Audio-blue) ![](https://img.shields.io/badge/Video-red)

- **Dialoguellm: Context and emotion knowledge-tuned large language models for emotion recognition in conversations** [[Neural Networks 2025]](https://arxiv.org/pdf/2310.11374) ![](https://img.shields.io/badge/Text-yellow) ![](https://img.shields.io/badge/Video-red)

- **EMO-Avatar: An¬†LLM-Agent-Orchestrated Framework for Multimodal¬†Emotional¬†Support in Human Animation** [[ACM MM 2025]](https://dl.acm.org/doi/pdf/10.1145/3746027.3762030?casa_token=r8wXM7kKSu0AAAAA:bvY66VbeNeoZqJYxHvG2HoRbqo0SbAhoeGfWbKnAvjqiac0xTVUcc_mdxrCxCNsW5n6sqZvh2F1QcYo) ![](https://img.shields.io/badge/Text-yellow) ![](https://img.shields.io/badge/Audio-blue) ![](https://img.shields.io/badge/Video-red)

- **Deemo: De-identity multimodal emotion recognition and reasoning** [[ACM MM 2025]](https://dl.acm.org/doi/pdf/10.1145/3746027.3755411) [[Code]](https://github.com/Leedeng/DEEMO) ![](https://img.shields.io/badge/Text-yellow) ![](https://img.shields.io/badge/Audio-blue) ![](https://img.shields.io/badge/Video-red)

- **Domain-Separated Bottleneck Attention Fusion Framework for Multimodal Emotion Recognition** [[ACM TMCCA 2025]](https://dl.acm.org/doi/pdf/10.1145/3711865?casa_token=8mc8yDePfmcAAAAA:H8XgThUxPK0iPsY7t4t0rGHLEoeWQelV3A7P90qs0eNNAAaXHLKNmExo0u1FPkXW7n8BQEHMe6GmwpY) ![](https://img.shields.io/badge/Text-yellow) ![](https://img.shields.io/badge/Audio-blue) ![](https://img.shields.io/badge/Video-red)

- **Emotion across Modalities and Cultures: Multilingual Multimodal Emotion-Cause Analysis with Memory-inspired Framework** [[ACM MM 2025]](https://dl.acm.org/doi/pdf/10.1145/3746027.3755655?casa_token=BAJ57exS8YYAAAAA:OcrIAhg7DzKMwWtGIgTewT266FaIHbaWn1Rd5RXKHD405mfrGsYWGZP16dvy-PMl5hNiHMNLTR-AqhE) ![](https://img.shields.io/badge/Text-yellow) ![](https://img.shields.io/badge/Audio-blue) ![](https://img.shields.io/badge/Video-red)

- **Benchmarking Zero-Shot Facial Emotion Annotation with Large Language Models: A Multi-Class and Multi-Frame Approach in DailyLife** [[Arxiv 2025]](https://arxiv.org/pdf/2502.12454) ![](https://img.shields.io/badge/Text-yellow) ![](https://img.shields.io/badge/Video-red)

- **Emotion-Qwen: Training Hybrid Experts for Unified Emotion and General Vision-Language Understanding** [[Arxiv 2025]](https://arxiv.org/pdf/2505.06685) [[Code]](https://anonymous.4open.science/r/Emotion-QwenAnonymous) ![](https://img.shields.io/badge/Text-yellow) ![](https://img.shields.io/badge/Video-red)

- **Towards multimodal emotional support conversation systems** [[TMM 2025]](https://ieeexplore.ieee.org/abstract/document/11146689) [[Code]](https://github.com/chuyq/MESC) ![](https://img.shields.io/badge/Text-yellow) ![](https://img.shields.io/badge/Audio-blue) ![](https://img.shields.io/badge/Video-red)

- **Omni-emotion: Extending video mllm with detailed face and audio modeling for multimodal emotion analysis** [[Arxiv 2025]](https://arxiv.org/pdf/2501.09502) [[Code]](https://github.com/HumanMLLM/Omni-Emotion) ![](https://img.shields.io/badge/Text-yellow) ![](https://img.shields.io/badge/Audio-blue) ![](https://img.shields.io/badge/Video-red)

- **Mer 2025: When affective computing meets large language models** [[ACM MM 2025]](https://dl.acm.org/doi/pdf/10.1145/3746027.3762007) [[Code]](https://github.com/zeroQiaoba/MERTools) ![](https://img.shields.io/badge/Text-yellow) ![](https://img.shields.io/badge/Audio-blue) ![](https://img.shields.io/badge/Video-red)

- **FaVChat: Unlocking Fine-Grained Facial Video Understanding with Multimodal Large Language Models** [[Arxiv 2025]](https://arxiv.org/pdf/2503.09158) ![](https://img.shields.io/badge/Text-yellow) ![](https://img.shields.io/badge/Video-red)

- **Humanomni: A large vision-speech language model for human-centric video understanding** [[Arxiv 2025]](https://arxiv.org/pdf/2501.15111?) [[Code]](https://github.com/HumanMLLM/HumanOmni) ![](https://img.shields.io/badge/Text-yellow) ![](https://img.shields.io/badge/Audio-blue) ![](https://img.shields.io/badge/Video-red)

- **Affective-CoT: Decomposing Multimodal Emotion Reasoning through a Hierarchical Cognitive Workflow** [[ACM MM 2025]](https://dl.acm.org/doi/pdf/10.1145/3746027.3762009?casa_token=wIgvmeIj2u0AAAAA:8-n-HMAxtO7VPFeAMylDhHDKZpkrMlND2xNOwn6h4LnYN5PVz7n_cKZYiFmVPMvG2mbaHlAoDYQnDpQ) ![](https://img.shields.io/badge/Text-yellow) ![](https://img.shields.io/badge/Audio-blue) ![](https://img.shields.io/badge/Video-red)

- **Emo-Synergy: Synergizing Task-Specific and General Experts for Multimodal Emotion Recognition** [[ACM MRAC 2025]](https://dl.acm.org/doi/pdf/10.1145/3746270.3760219?casa_token=ezmNSByXb7UAAAAA:5kfmf5Q73f0Kg1bNcKX8q_cpNwqJ3d9YOVthVOqjoHyn1NRsOu5Ff1dWJcO96RCMg4jGBbdESsgUOSg) ![](https://img.shields.io/badge/Text-yellow) ![](https://img.shields.io/badge/Audio-blue) ![](https://img.shields.io/badge/Video-red)

- **ROSA: A Robust Self-Adaptive Model for Multimodal Emotion Recognition With Uncertain Missing Modalities** [[TMM 2025]](https://ieeexplore.ieee.org/abstract/document/11086418) [[Code]](https://github.com/gw-zhong/cider) ![](https://img.shields.io/badge/Text-yellow) ![](https://img.shields.io/badge/Audio-blue) ![](https://img.shields.io/badge/Video-red)

- **HumanOmniV2: From Understanding to Omni-Modal Reasoning with Context** [[Arxiv 2025]](https://arxiv.org/pdf/2506.21277) [[Code]](https://github.com/HumanMLLM/HumanOmniV2) ![](https://img.shields.io/badge/Text-yellow) ![](https://img.shields.io/badge/Audio-blue) ![](https://img.shields.io/badge/Video-red)

- **R1-Omni: Explainable Omni-Multimodal Emotion Recognition with Reinforcement Learning** [[Arxiv 2025]](https://arxiv.org/abs/2503.05379) [[Code]](https://github.com/HumanMLLM/R1-Omni) ![](https://img.shields.io/badge/Text-yellow) ![](https://img.shields.io/badge/Video-red)

- **Speak From Heart: An Emotion-Guided LLM-Based Multimodal Method for Emotional Dialogue Generation** [[ICMR 2024]](https://dl.acm.org/doi/pdf/10.1145/3652583.3658104?casa_token=P5hl3kvvBl4AAAAA:QfrNowiUuZ_nUS5NlzLm0I_EGjcfXhco6uG49PmUz3iaLsx1ZWnfPI5JP24ICys_H4jtkRDKcJLX14k) ![](https://img.shields.io/badge/Text-yellow) ![](https://img.shields.io/badge/Audio-blue) ![](https://img.shields.io/badge/Video-red)

- **A LLM-Based Robot Partner with Multi-modal Emotion Recognition** [[ICIRA 2024]](https://link.springer.com/chapter/10.1007/978-981-96-0786-0_6) ![](https://img.shields.io/badge/Text-yellow) ![](https://img.shields.io/badge/Video-red)

- **Multimodal Emotion Recognition Using Feature Fusion: An LLM-Based Approach** [[IEEE Access 2024]](https://ieeexplore.ieee.org/iel8/6287639/6514899/10591796.pdf) ![](https://img.shields.io/badge/Text-yellow) ![](https://img.shields.io/badge/Audio-blue) ![](https://img.shields.io/badge/Video-red)

- **Emo-llama: Enhancing facial emotion understanding with instruction tuning** [[Arxiv 2024]](https://arxiv.org/pdf/2408.11424) [[Code]](https://github.com/xxtars/EMO-LLaMA) ![](https://img.shields.io/badge/Text-yellow) ![](https://img.shields.io/badge/Video-red)

- **Emotion-llama: Multimodal emotion recognition and reasoning with instruction tuning** [[NeurIPS 2024]](https://proceedings.neurips.cc/paper_files/paper/2024/file/c7f43ada17acc234f568dc66da527418-Paper-Conference.pdf) [[Code]](https://github.com/ZebangCheng/Emotion-LLaMA) ![](https://img.shields.io/badge/Text-yellow) ![](https://img.shields.io/badge/Video-red)

- **SZTU-CMU at MER2024: Improving Emotion-LLaMA with Conv-Attention for Multimodal Emotion Recognition** [[ACM MRAC 2024]](https://dl.acm.org/doi/pdf/10.1145/3689092.3689404?casa_token=ra-epwYeekIAAAAA:p7GyrNJWXb8yT2pS_YZAFmQLa8BiueIMrvY-VfL9mCDoRsigWmy5wfWSssjC90enK1s9Vw18KVItLvE) [[Code]](https://github.com/ZebangCheng/Emotion-LLaMA) ![](https://img.shields.io/badge/Text-yellow) ![](https://img.shields.io/badge/Audio-blue) ![](https://img.shields.io/badge/Video-red)

- **Eald-mllm: Emotion analysis in long-sequential and de-identity videos with multi-modal large language model** [[Arxiv 2024]](https://arxiv.org/pdf/2405.00574?) ![](https://img.shields.io/badge/Text-yellow) ![](https://img.shields.io/badge/Audio-blue) ![](https://img.shields.io/badge/Video-red)

- **Textualized and feature-based models for compound multimodal emotion recognition in the wild** [[ECCVW 2024]](https://arxiv.org/pdf/2407.12927?) [[Code]](https://github.com/sbelharbi/feature-vs-text-compound-emotion) ![](https://img.shields.io/badge/Text-yellow) ![](https://img.shields.io/badge/Audio-blue) ![](https://img.shields.io/badge/Video-red)

- **AV-EmoDialog: Chat with Audio-Visual Users Leveraging Emotional Cues** [[Arxiv 2024]](https://arxiv.org/pdf/2412.17292) ![](https://img.shields.io/badge/Text-yellow) ![](https://img.shields.io/badge/Audio-blue) ![](https://img.shields.io/badge/Video-red)

- **Vllms provide better context for emotion understanding through common sense reasoning** [[Arxiv 2024]](https://arxiv.org/pdf/2404.07078) ![](https://img.shields.io/badge/Text-yellow) ![](https://img.shields.io/badge/Video-red)

- **Multimodal Emotion Captioning Using Large Language Model with Prompt Engineering** [[ACM MRAC 2024]](https://dl.acm.org/doi/pdf/10.1145/3689092.3689403) ![](https://img.shields.io/badge/Text-yellow) ![](https://img.shields.io/badge/Audio-blue) ![](https://img.shields.io/badge/Video-red)

- **MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Subtle Clue Dynamics in Video Dialogues** [[ACM MRAC 2024]](https://dl.acm.org/doi/pdf/10.1145/3689092.3689408?casa_token=rjmQ42IIa6oAAAAA:MmgVG1f3dTjYXd5ZJ9ETczxq2Ohw0VUywt1sg0TIagOxcvlPtQ0FsP3zkYd8GqR_C-kVl5Uf9-10Ak4) ![](https://img.shields.io/badge/Text-yellow) ![](https://img.shields.io/badge/Audio-blue) ![](https://img.shields.io/badge/Video-red)

- **E3: Exploring Embodied Emotion Through A Large-Scale Egocentric Video Dataset** [[NeurIPS 2024]](https://proceedings.neurips.cc/paper_files/paper/2024/file/d611d5c0251d9680f869c5d2c46c6fcd-Paper-Datasets_and_Benchmarks_Track.pdf) [[Code]](https://github.com/Exploring-Embodied-Emotion-official/E3) ![](https://img.shields.io/badge/Text-yellow) ![](https://img.shields.io/badge/Audio-blue) ![](https://img.shields.io/badge/Video-red)

- **Video emotion open-vocabulary recognition based on multimodal large language model** [[Arxiv 2024]](https://arxiv.org/pdf/2408.11286?) ![](https://img.shields.io/badge/Text-yellow) ![](https://img.shields.io/badge/Audio-blue) ![](https://img.shields.io/badge/Video-red)

## ‚≠ê Image MLLM

- **MODA: MOdular Duplex Attention for Multimodal Perception, Cognition, and Emotion Understanding** [[ICML 2025]](https://zzcheng.top/assets/pdf/2025_ICML_MODA.pdf) [[Code]](https://github.com/KlingTeam/MODA) ![](https://img.shields.io/badge/Image-green) ![](https://img.shields.io/badge/Text-yellow)

- **Why We Feel: Breaking Boundaries in Emotional Reasoning with Multimodal Large Language Models** [[CVPR 2025]](https://openaccess.thecvf.com/content/CVPR2025W/NeXD/papers/Lin_Why_We_Feel_Breaking_Boundaries_in_Emotional_Reasoning_with_Multimodal_CVPRW_2025_paper.pdf) [[Code]](https://github.com/Lum1104/EIBench) ![](https://img.shields.io/badge/Image-green) ![](https://img.shields.io/badge/Text-yellow)

- **Empathic Prompting: Non-Verbal Context Integration for Multimodal LLM Conversations** [[Arxiv 2025]](https://arxiv.org/pdf/2510.20743) ![](https://img.shields.io/badge/Image-green) ![](https://img.shields.io/badge/Text-yellow)

- **Enhancing Zero-Shot Facial Expression Recognition by LLM Knowledge Transfer** [[WACV 2025]](https://arxiv.org/pdf/2405.19100) [[Code]](https://github.com/zengqunzhao/Exp-CLIP) ![](https://img.shields.io/badge/Image-green) ![](https://img.shields.io/badge/Text-yellow)

- **Exploring LLM-generated culture-specific affective human-robot tactile interaction** [[Arxiv 2025]](https://arxiv.org/pdf/2507.22905) ![](https://img.shields.io/badge/Image-green) ![](https://img.shields.io/badge/Text-yellow)

- **Emosym: A symbiotic framework for unified emotional understanding and generation via latent reasoning** [[ACM MM 2025]](https://dl.acm.org/doi/pdf/10.1145/3746027.3754549?casa_token=q88KWCCQ0PkAAAAA:0oWoMaOu6sZL2fSz56UF-aICQgNZ042lHubR_Btd04VsLilkfRKuWDAuTcrt31no3tyPcIvN4SnMB84) [[Code]](https://github.com/JiuTian-VL/EmoSym) ![](https://img.shields.io/badge/Image-green) ![](https://img.shields.io/badge/Text-yellow)

- **Understanding emotional body expressions via large language models** [[AAAI 2025]](https://ojs.aaai.org/index.php/AAAI/article/download/32135/34290) ![](https://img.shields.io/badge/Image-green) ![](https://img.shields.io/badge/Text-yellow)

- **Feallm: Advancing facial emotion analysis in multimodal large language models with emotional synergy and reasoning** [[ACM MM 2025]](https://dl.acm.org/doi/pdf/10.1145/3746027.3755352?casa_token=9X93eH2a15IAAAAA:I_jhSJMahTxoWSZPowJTHZDhj1Bwez-Yfq_c-ZCL5wsoIBX6gyfpA-Fc3DOilmLu0MtiXLDDi6HtfcQ) [[Code]](https://github.com/953206211/FEALLM) ![](https://img.shields.io/badge/Image-green) ![](https://img.shields.io/badge/Text-yellow)

- **MELT: Towards Automated Multimodal Emotion Data Annotation by Leveraging LLM Embedded Knowledge** [[Arxiv 2025]](https://arxiv.org/pdf/2505.24493?) [[Code]](https://github.com/KeiKinn/meltdataset) ![](https://img.shields.io/badge/Image-green) ![](https://img.shields.io/badge/Text-yellow)

- **Rethinking Facial Expression Recognition in the Era of Multimodal Large Language Models: Benchmark, Datasets, and Beyond** [[Arxiv 2025]](https://arxiv.org/pdf/2511.00389) [[Code]](https://github.com/zfkarl/UniFER) ![](https://img.shields.io/badge/Image-green) ![](https://img.shields.io/badge/Text-yellow)

- **DEFT-LLM: Disentangled Expert Feature Tuning for Micro-Expression Recognition** [[Arxiv 2025]](https://arxiv.org/pdf/2511.10948) ![](https://img.shields.io/badge/Image-green) ![](https://img.shields.io/badge/Text-yellow)

- **Mellm: Exploring llm-powered micro-expression understanding enhanced by subtle motion perception** [[Arxiv 2025]](https://arxiv.org/pdf/2505.07007) ![](https://img.shields.io/badge/Image-green) ![](https://img.shields.io/badge/Text-yellow)

- **Contextual Attention-Based Multimodal Fusion of¬†LLM¬†and CNN for Sentiment Analysis** [[Arxiv 2025]](https://arxiv.org/pdf/2508.13196?) ![](https://img.shields.io/badge/Image-green) ![](https://img.shields.io/badge/Text-yellow)

- **FaceXBench: Evaluating Multimodal LLMs on Face Understanding** [[Arxiv 2025]](https://arxiv.org/pdf/2501.10360) [[Code]](https://github.com/Kartik-3004/facexbench) ![](https://img.shields.io/badge/Image-green) ![](https://img.shields.io/badge/Text-yellow)

- **Beyond Vision: How Large Language Models Interpret Facial Expressions from Valence-Arousal Values** [[Arxiv 2025]](https://arxiv.org/pdf/2502.06875) ![](https://img.shields.io/badge/Image-green) ![](https://img.shields.io/badge/Text-yellow)

- **Unif2ace: Fine-grained Face Understanding and Generation with Unified Multimodal Models** [[Arxiv 2025]](https://arxiv.org/pdf/2503.08120?) [[Code]](https://github.com/tulvgengenr/UniF2ace) ![](https://img.shields.io/badge/Image-green) ![](https://img.shields.io/badge/Text-yellow)

- **FaceInsight: A Multimodal Large Language Model for Face Perception** [[ACM MM 2025]](https://dl.acm.org/doi/pdf/10.1145/3746027.3755634) ![](https://img.shields.io/badge/Image-green) ![](https://img.shields.io/badge/Text-yellow)

- **AMPLE: Emotion-aware multimodal fusion prompt learning for fake news detection** [[MMM 2025]](https://arxiv.org/pdf/2410.15591) ![](https://img.shields.io/badge/Image-green) ![](https://img.shields.io/badge/Text-yellow)

- **Customizing Visual Emotion Evaluation for MLLMs: An Open-vocabulary, Multifaceted, and Scalable Approach** [[Arxiv 2025]](https://arxiv.org/pdf/2509.21950) [[Code]](https://github.com/wdqqdw/MVEI) ![](https://img.shields.io/badge/Image-green) ![](https://img.shields.io/badge/Text-yellow)

- **Facial Emotion Detection Research based on an Improved Multi-modal LLM** [[AIHCIR 2024]](https://ieeexplore.ieee.org/abstract/document/10974272/) ![](https://img.shields.io/badge/Image-green) ![](https://img.shields.io/badge/Text-yellow)

- **Face-mllm: A large face perception model** [[Arxiv 2024]](https://arxiv.org/pdf/2410.20717?) ![](https://img.shields.io/badge/Image-green) ![](https://img.shields.io/badge/Text-yellow)

- **EmoLLM: Multimodal Emotional Understanding Meets Large Language Models** [[Arxiv 2024]](https://arxiv.org/pdf/2406.16442?) [[Code]](https://github.com/yan9qu/EmoLLM) ![](https://img.shields.io/badge/Image-green) ![](https://img.shields.io/badge/Text-yellow)

- **EVOLVE: Emotion and Visual Output Learning via LLM Evaluation** [[Arxiv 2024]](https://arxiv.org/pdf/2412.20632) ![](https://img.shields.io/badge/Image-green) ![](https://img.shields.io/badge/Text-yellow)

- **Knowledge-based emotion recognition using large language models** [[ACII 2024]](https://arxiv.org/pdf/2408.04123) ![](https://img.shields.io/badge/Image-green) ![](https://img.shields.io/badge/Text-yellow)

- **Vision-enabled large language and deep learning models for image-based emotion recognition** [[Cognitive Computation 2024]](https://link.springer.com/article/10.1007/s12559-024-10281-5) ![](https://img.shields.io/badge/Image-green) ![](https://img.shields.io/badge/Text-yellow)

- **Contextual emotion recognition using large vision language models** [[IROS 2024]](https://arxiv.org/pdf/2405.08992) [[Code]](https://github.com/yasaman-etesam/Contextual-Emotion-Recognition) ![](https://img.shields.io/badge/Image-green) ![](https://img.shields.io/badge/Text-yellow)

- **Analyzing key factors influencing emotion prediction performance of vllms in conversational contexts** [[ACL 2024]](https://aclanthology.org/2024.emnlp-main.331.pdf) ![](https://img.shields.io/badge/Image-green) ![](https://img.shields.io/badge/Text-yellow)


## ‚≠ê Audio MLLM

- **LLM supervised Pre-training for Multimodal Emotion Recognition in Conversations** [[ICASSP 2025]](https://arxiv.org/pdf/2501.11468) ![](https://img.shields.io/badge/Text-yellow) ![](https://img.shields.io/badge/Audio-blue)

- **EmoVoice: LLM-based Emotional Text-To-Speech Model with Freestyle Text Prompting** [[ACM MM 2025]](https://dl.acm.org/doi/pdf/10.1145/3746027.3754829?casa_token=TnIbuRTJhXgAAAAA:h_hFvwr6Gad0KgI8rfFwXHnKM47T1jc4TWliVJfBD6tM9LbJw2zltKWDBZ4dCWf5TpqJfm8DTZ1K12c) [[Code]](https://github.com/yanghaha0908/EmoVoice) ![](https://img.shields.io/badge/Text-yellow) ![](https://img.shields.io/badge/Audio-blue)

- **AER-LLM: Ambiguity-aware emotion recognition leveraging large language models** [[ICASSP 2025]](https://arxiv.org/pdf/2409.18339) [[Code]](https://github.com/mHealthUnimelb/AER-LLM) ![](https://img.shields.io/badge/Text-yellow) ![](https://img.shields.io/badge/Audio-blue)

- **EmoJudge: LLM Based Post-Hoc Refinement for Multimodal Speech Emotion Recognition** [[Interspeech 2025]](https://prabhav55221.github.io/file/EmoJudge_Interspeech_CameraReady.pdf) ![](https://img.shields.io/badge/Text-yellow) ![](https://img.shields.io/badge/Audio-blue)

- **Prompt-Unseen-Emotion: Zero-shot Expressive Speech Synthesis with Prompt-LLM Contextual Knowledge for Mixed Emotions** [[Arxiv 2025]](https://arxiv.org/pdf/2506.02742) ![](https://img.shields.io/badge/Text-yellow) ![](https://img.shields.io/badge/Audio-blue)

- **Dynamic Parameter Memory: Temporary LoRA-Enhanced LLM for Long-Sequence Emotion Recognition in Conversation** [[Arxiv 2025]](https://arxiv.org/pdf/2507.09076?) ![](https://img.shields.io/badge/Text-yellow) ![](https://img.shields.io/badge/Audio-blue)

- **RLAIF-SPA: Optimizing LLM-based Emotional Speech Synthesis via RLAIF** [[Arxiv 2025]](https://arxiv.org/pdf/2510.14628) [[Code]](https://github.com/Zoe-Mango/RLAIF-SPA) ![](https://img.shields.io/badge/Text-yellow) ![](https://img.shields.io/badge/Audio-blue)

- **Towards¬†LLM-Empowered Fine-Grained Speech Descriptors for Explainable¬†Emotion¬†Recognition** [[Arxiv 2025]](https://arxiv.org/pdf/2505.23236) ![](https://img.shields.io/badge/Text-yellow) ![](https://img.shields.io/badge/Audio-blue)

- **EmoSLLM: Parameter-Efficient Adaptation of LLMs for Speech Emotion Recognition** [[Arxiv 2025]](https://arxiv.org/pdf/2508.14130) ![](https://img.shields.io/badge/Text-yellow) ![](https://img.shields.io/badge/Audio-blue)

- **EMORL-TTS: Reinforcement Learning for Fine-Grained Emotion Control in LLM-based TTS** [[Arxiv 2025]](https://arxiv.org/pdf/2510.05758) ![](https://img.shields.io/badge/Text-yellow) ![](https://img.shields.io/badge/Audio-blue)

- **Chain-of-Thought Distillation with Fine-Grained Acoustic Cues for Speech Emotion Recognition** [[Interspeech 2025]](https://www.isca-archive.org/interspeech_2025/mai25c_interspeech.pdf) ![](https://img.shields.io/badge/Text-yellow) ![](https://img.shields.io/badge/Audio-blue)

- **Toward zero-shot speech emotion recognition using llms in the absence of target data** [[ICASSP 2025]](https://www.researchgate.net/profile/Shreya-Upadhyay-10/publication/390538877_Toward_Zero-Shot_Speech_Emotion_Recognition_Using_LLMs_in_the_Absence_of_Target_Data/links/6813277cded433155740b2ed/Toward-Zero-Shot-Speech-Emotion-Recognition-Using-LLMs-in-the-Absence-of-Target-Data.pdf) ![](https://img.shields.io/badge/Text-yellow) ![](https://img.shields.io/badge/Audio-blue)

- **Enhanced Emotion-aware Music Recommendation via Large Language Models** [[KDD 2025]](https://dl.acm.org/doi/pdf/10.1145/3711896.3737212?casa_token=tiiDDWK6OI0AAAAA:6xNktRTNeBTi7zqYJcgSWHpsDEFbVklqKUfDtLFkEIJZhedfNpn3R8z01xPntbW42X7GeXyinapG8Vo) ![](https://img.shields.io/badge/Text-yellow) ![](https://img.shields.io/badge/Audio-blue)

- **Emo-dpo: Controllable emotional speech synthesis through direct preference optimization** [[ICASSP 2025]](https://arxiv.org/pdf/2409.10157) ![](https://img.shields.io/badge/Text-yellow) ![](https://img.shields.io/badge/Audio-blue)

- **Beyond Global Emotion: Fine-Grained Emotional Speech Synthesis with Dynamic Word-Level Modulation** [[Arxiv 2025]](https://arxiv.org/pdf/2509.20378?) ![](https://img.shields.io/badge/Text-yellow) ![](https://img.shields.io/badge/Audio-blue)

- **SMIIP-NV: A Multi-Annotation Non-Verbal Expressive Speech Corpus in Mandarin for LLM-Based Speech Synthesis** [[ACM MM 2025]](https://dl.acm.org/doi/pdf/10.1145/3746027.3758312?casa_token=ckeXSB_6OsQAAAAA:zuTMuE_5xR2Tk7DifB7tL2myAoGvvEBNureA7sb8E1V2GXYahLvWgvtv_Wegu5bQ5AnENWUIl8R3cEM) ![](https://img.shields.io/badge/Text-yellow) ![](https://img.shields.io/badge/Audio-blue)

- **Leveraging speech ptm, text llm, and emotional tts for speech emotion recognition** [[ICASSP 2024]](https://arxiv.org/pdf/2309.10294) ![](https://img.shields.io/badge/Text-yellow) ![](https://img.shields.io/badge/Audio-blue)

- **Large language model-based¬†emotional¬†speech annotation using context and acoustic feature for speech¬†emotion¬†recognition** [[ICASSP 2024]](https://ieeexplore.ieee.org/abstract/document/10448316/) ![](https://img.shields.io/badge/Text-yellow) ![](https://img.shields.io/badge/Audio-blue)

- **Enhancing multimodal emotion recognition through asr error compensation and llm fine-tuning** [[Interspeech 2024]](https://www.isca-archive.org/interspeech_2024/kyung24_interspeech.pdf) ![](https://img.shields.io/badge/Text-yellow) ![](https://img.shields.io/badge/Audio-blue)

- **LanSER: Language-Model Supported Speech Emotion Recognition** [[Interspeech 2023]](https://arxiv.org/pdf/2309.03978) ![](https://img.shields.io/badge/Text-yellow) ![](https://img.shields.io/badge/Audio-blue)
